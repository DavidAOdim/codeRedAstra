# ThermaMind Dashboard - Real-World Context

## ğŸ¢ Data Center Structure

### Clusters (4 total: A, B, C, D)
A **cluster** is a physical group of 8 GPU servers working together in the same location.
- **Location**: Physical data center (Houston, Dallas, Austin)
- **Workload**: Type of AI work (Training, Inference, Development)
- **Contains**: 8 individual GPU nodes

### Nodes (32 total: 8 per cluster)
A **node** is a single physical GPU server.
- **Label**: A1-A8, B1-B8, C1-C8, D1-D8
- **Each node tracks**: GPU load, temperature, cooling, power consumption

---

## ğŸ“Š Metrics Explained

### GPU Load (0-100%)
How hard the GPU is working.
- **0-30%**: Idle (barely doing anything)
- **30-75%**: Active (normal workload)
- **75-100%**: Hot (heavy workload)

### Temperature (Â°C)
Physical heat generated by the GPU chip.
- **Correlates with GPU Load**: Higher load = higher temperature
- **Typical range**: 20-50Â°C
- **Offline nodes**: 0Â°C (no power = no heat)

### Cooling (0-100%)
Percentage of active cooling system capacity needed.

**Real-world physics:**
- GPU processes data â†’ Consumes electricity â†’ Generates heat
- Heat must be removed or GPU will overheat (damage at ~90Â°C)
- Cooling systems (CRAC units, liquid cooling) remove heat
- More GPU work = More heat = More cooling needed

**Traditional data centers (wasteful):**
- Run cooling at constant 80-100% regardless of load
- "Better safe than sorry" mentality
- GPU at 30% load? Still cooling at 95%! â† Wastes 65% cooling energy

**AI optimization (smart):**
- Monitors actual GPU load and temperature
- Adjusts cooling to match real needs + 5-10% safety buffer
- GPU at 30%? Cooling at 35% â† Saves 60% energy!
- GPU spikes to 80%? Pre-cooling kicks in before temp rises

**In the code:**
- **Ideal cooling** = GPU load + 5% (safety margin)
- **Traditional over-cooling** = GPU load + 15-30% (wasteful but common)
- **AI-optimized** = 70% of nodes run near-ideal, 30% still catching up from over-cooling
- Shows realistic system adjusting over time, not instant perfection

### Power (kW)
Electricity consumed by the node or cluster.
- **Node power**: Individual GPU + cooling (typically 5-20 kW per node)
- **Cluster power**: Sum of all 8 nodes (shown in cluster list)
- **Data center total**: Sum of all clusters (shown in top stats)

---

## ğŸ¨ Visual States

### Grid Heatmap Colors
- ğŸ”´ **Red (Hot)**: >75% GPU load - Heavy workload, high temperature
- ğŸŸ¢ **Green (Active)**: 30-75% GPU load - Normal operation
- âš« **Gray (Idle)**: <30% GPU load OR offline - Minimal/no activity

### Cluster Status
Determined by **GPU load + cooling efficiency**:

#### ğŸ”¥ ACTIVE
**What it means**: Cluster is running heavy workloads
- **When**: Average GPU load > 70%
- **Example**: Training large AI models, running inference at scale
- **What you see**: Green/red nodes in the grid, high power consumption
- **AI doing**: Ensuring cooling keeps up with demand

#### âš™ï¸ OPTIMIZING
**What it means**: AI is adjusting cooling to save energy
- **When**: Cooling is mismatched with actual GPU load (15%+ difference)
- **Examples**:
  - GPU at 60%, but cooling at 95% â†’ AI reduces cooling to 65%
  - GPU at 45%, cooling at 30% â†’ AI increases cooling slightly
- **What you see**: Mixed node colors, cooling % adjusting
- **AI doing**: Dynamically balancing cooling to match workload
- **Result**: Same performance, less wasted energy

#### ğŸ’¤ IDLE
**What it means**: Cluster has minimal workload
- **When**: Average GPU load < 40% AND cooling is appropriate
- **Example**: Development cluster overnight, no active jobs
- **What you see**: Mostly gray nodes in grid, low power draw
- **AI doing**: Keeping cooling minimal to save maximum energy
- **Result**: Ready for work but not wasting electricity

---

## ğŸ¤– What the AI Does

### Cooling Optimization
- Detects: GPU at 60% but cooling at 95%
- Action: Reduces cooling to 65%, saving 124 kW
- Result: Same performance, less energy

### Workload Reallocation
- Detects: Cluster D idle for 23 minutes
- Action: Migrates training jobs from busy Cluster A to D
- Result: Better resource utilization, lower peak power

### Predictive Pre-cooling
- Detects: Forecasting 40% load increase in 15 minutes
- Action: Pre-cools systems before spike hits
- Result: Maintains efficiency during peaks

---

## ğŸ¯ Real-World Cooling Scenarios

### Scenario 1: Traditional Data Center (Wasteful)
**Problem**: Static cooling setpoints
```
Time    GPU Load    Cooling     Waste
00:00   80%        95%         15% wasted
02:00   45%        95%         50% wasted! âš ï¸
04:00   30%        95%         65% wasted! âš ï¸
08:00   75%        95%         20% wasted
```
**Annual cost**: $2.4M in electricity
**Why?**: "Set it high and forget it" - fearful of overheating

### Scenario 2: With AI Optimization (Smart)
**Solution**: Dynamic cooling adjustment
```
Time    GPU Load    Cooling     Efficiency
00:00   80%        85%         Only 5% buffer âœ…
02:00   45%        50%         Saved 45%! âœ…
04:00   30%        35%         Saved 60%! âœ…
08:00   75%        80%         Only 5% buffer âœ…
```
**Annual cost**: $1.5M in electricity
**Savings**: $900K/year = 37.5% reduction! ğŸ’°

### How ThermaMind AI Works:

1. **Monitors**: Tracks GPU load every 2 seconds across all 32 nodes
2. **Predicts**: "GPU load trending up? Pre-cool before it gets hot"
3. **Adjusts**: Reduces cooling when load drops, increases when load rises
4. **Learns**: "Every Tuesday 2pm = peak training jobs, start pre-cooling at 1:45pm"

**Physical constraints respected:**
- Cooling can't change instantly (CRAC units have ~30 second lag)
- Safety margin always maintained (never drop below GPU load + 5%)
- Temperature monitored - if rising too fast, override and cool more

---

## ï¿½ Data Flow

1. **Backend generates**: 4 clusters, each with 8 nodes
2. **Each node has**: Individual GPU load, temp, cooling, power
3. **Cluster aggregates**: 
   - GPU Load = Average of 8 nodes
   - Cooling = Average of 8 nodes
   - Power = **Sum** of 8 nodes (power adds up!)
4. **Frontend displays**:
   - Cluster list: Aggregated stats
   - Grid heatmap: Individual node states
   - Charts: Trends over time

---

## ğŸ¯ Key Insights

### Why This Matters
Traditional data centers run cooling at full capacity "just in case". ThermaMind's AI:
- Monitors real-time GPU usage across all 32 nodes
- Adjusts cooling dynamically per cluster
- Saves 30-40% energy while maintaining performance
- Reduces COâ‚‚ emissions equivalent to planting trees

### Example Scenario
**Cluster A**: Training AI models (high GPU load = red nodes)
**Cluster B**: Running inference (medium load = green nodes)
**Cluster C**: Development testing (mixed workload)
**Cluster D**: Idle overnight (gray nodes = save energy)

The AI ensures Cluster D isn't being cooled as if it's working at full capacity!
